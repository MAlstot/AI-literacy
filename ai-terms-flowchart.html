<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Key Terms: How They Fit Together</title>
<style>
  :root {
    --green: #154734;
    --gold: #FFB81C;
    --gold-dark: #c48f00;
    --gold-light: #fff5d6;
    --green-light: #e8f0ec;
    --ink: #1a1a1a;
    --mid: #595959;
    --paper: #f8f6f0;
    --white: #ffffff;
    --border: #d0ccc4;

    /* Phase colors */
    --phase-build: #154734;
    --phase-run: #1a6b8a;
    --phase-output: #7a3a00;
    --phase-oversight: #6b2d6b;
  }

  *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

  body {
    background: var(--paper);
    font-family: Georgia, 'Times New Roman', serif;
    color: var(--ink);
    font-size: 15px;
    line-height: 1.6;
    padding: 28px 16px 48px;
  }

  .skip-link {
    position: absolute; left: -9999px; top: 0;
    background: var(--green); color: white;
    padding: 8px 16px; font-family: Arial, sans-serif;
    font-size: 0.9rem; z-index: 9999;
    text-decoration: none;
  }
  .skip-link:focus { left: 0; }

  .page { max-width: 860px; margin: 0 auto; }

  /* HEADER */
  .header {
    text-align: center;
    margin-bottom: 28px;
    padding-bottom: 20px;
    border-bottom: 3px solid var(--gold);
  }
  .header h1 {
    font-family: Georgia, serif;
    font-size: 1.8rem;
    color: var(--green);
    margin-bottom: 8px;
  }
  .header p {
    font-family: Arial, sans-serif;
    font-size: 0.92rem;
    color: var(--mid);
    max-width: 560px;
    margin: 0 auto;
  }

  .hint {
    text-align: center;
    font-family: Arial, sans-serif;
    font-size: 0.82rem;
    color: var(--mid);
    font-style: italic;
    margin-bottom: 24px;
  }

  /* PHASE STRIP */
  .phase-strip {
    display: flex;
    gap: 0;
    margin-bottom: 0;
    border-radius: 6px 6px 0 0;
    overflow: hidden;
  }

  .phase-label {
    flex: 1;
    padding: 10px 8px;
    text-align: center;
    font-family: Arial, sans-serif;
    font-size: 0.75rem;
    font-weight: bold;
    letter-spacing: 0.07em;
    text-transform: uppercase;
    color: white;
  }

  .phase-label.build    { background: var(--phase-build); }
  .phase-label.run      { background: var(--phase-run); }
  .phase-label.output   { background: var(--phase-output); }
  .phase-label.oversight { background: var(--phase-oversight); }

  /* FLOWCHART AREA */
  .flowchart-area {
    background: var(--white);
    border: 2px solid var(--border);
    border-top: none;
    border-radius: 0 0 6px 6px;
    padding: 28px 20px 20px;
    margin-bottom: 24px;
    overflow-x: auto;
  }

  /* PHASE COLUMNS */
  .flow-columns {
    display: flex;
    gap: 0;
    align-items: stretch;
    min-width: 600px;
  }

  .flow-col {
    flex: 1;
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 12px;
    padding: 0 8px;
    position: relative;
  }

  /* Vertical connector between columns */
  .flow-col:not(:last-child)::after {
    content: '‚Üí';
    position: absolute;
    right: -10px;
    top: 50%;
    transform: translateY(-50%);
    font-size: 1.4rem;
    color: var(--border);
    font-family: Arial, sans-serif;
    z-index: 2;
  }

  /* TERM PILL */
  .term-pill {
    width: 100%;
    padding: 10px 12px;
    border-radius: 6px;
    border: 2px solid var(--border);
    background: var(--white);
    font-family: Arial, sans-serif;
    font-size: 0.82rem;
    font-weight: bold;
    color: var(--ink);
    cursor: pointer;
    text-align: center;
    transition: box-shadow 0.2s, transform 0.15s, border-color 0.2s;
    position: relative;
  }

  .term-pill:hover, .term-pill:focus {
    outline: 3px solid var(--gold);
    outline-offset: 2px;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }

  .term-pill.active {
    border-width: 3px;
    transform: translateY(-2px);
    box-shadow: 0 4px 16px rgba(0,0,0,0.15);
  }

  .term-pill.dimmed {
    opacity: 0.25;
  }

  .term-pill.connected {
    border-width: 3px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  }

  /* Phase-colored borders */
  .term-pill.build    { border-color: var(--phase-build); }
  .term-pill.run      { border-color: var(--phase-run); }
  .term-pill.output   { border-color: var(--phase-output); }
  .term-pill.oversight { border-color: var(--phase-oversight); }

  .term-pill.build.active    { background: #e8f0ec; }
  .term-pill.run.active      { background: #e0f0f7; }
  .term-pill.output.active   { background: #f7ece0; }
  .term-pill.oversight.active { background: #f3e0f3; }

  .term-pill.build.connected    { background: #e8f0ec; border-color: var(--phase-build); }
  .term-pill.run.connected      { background: #e0f0f7; border-color: var(--phase-run); }
  .term-pill.output.connected   { background: #f7ece0; border-color: var(--phase-output); }
  .term-pill.oversight.connected { background: #f3e0f3; border-color: var(--phase-oversight); }

  /* Phase tag inside pill */
  .pill-phase {
    display: block;
    font-size: 0.62rem;
    font-weight: normal;
    letter-spacing: 0.05em;
    text-transform: uppercase;
    color: var(--mid);
    margin-bottom: 2px;
  }

  /* DETAIL PANEL */
  .detail-panel {
    background: var(--gold-light);
    border-left: 4px solid var(--gold);
    border-radius: 4px;
    padding: 20px 22px;
    margin-bottom: 24px;
    min-height: 120px;
    display: none;
  }
  .detail-panel.visible { display: block; }

  .detail-panel h2 {
    font-family: Georgia, serif;
    font-size: 1.1rem;
    color: var(--green);
    margin-bottom: 6px;
  }
  .detail-panel .detail-phase {
    display: inline-block;
    font-family: Arial, sans-serif;
    font-size: 0.7rem;
    font-weight: bold;
    letter-spacing: 0.07em;
    text-transform: uppercase;
    padding: 2px 10px;
    border-radius: 3px;
    margin-bottom: 10px;
    color: white;
  }
  .detail-panel .detail-phase.build    { background: var(--phase-build); }
  .detail-panel .detail-phase.run      { background: var(--phase-run); }
  .detail-panel .detail-phase.output   { background: var(--phase-output); }
  .detail-panel .detail-phase.oversight { background: var(--phase-oversight); }

  .detail-panel .definition {
    font-family: Arial, sans-serif;
    font-size: 0.9rem;
    color: var(--ink);
    line-height: 1.7;
    margin-bottom: 10px;
  }
  .detail-panel .connections-label {
    font-family: Arial, sans-serif;
    font-size: 0.78rem;
    font-weight: bold;
    text-transform: uppercase;
    letter-spacing: 0.06em;
    color: var(--green);
    margin-bottom: 6px;
  }
  .detail-panel .connections-list {
    font-family: Arial, sans-serif;
    font-size: 0.85rem;
    color: var(--mid);
    line-height: 1.6;
  }

  /* PHASE LEGEND */
  .phase-legend {
    display: flex;
    gap: 16px;
    flex-wrap: wrap;
    justify-content: center;
    margin-bottom: 28px;
  }
  .legend-item {
    display: flex;
    align-items: center;
    gap: 8px;
    font-family: Arial, sans-serif;
    font-size: 0.8rem;
    color: var(--mid);
  }
  .legend-dot {
    width: 14px; height: 14px;
    border-radius: 3px;
    flex-shrink: 0;
  }

  /* KEY CONCEPTS */
  .concepts { margin-top: 8px; }
  .concepts h2 {
    font-family: Georgia, serif;
    font-size: 1.15rem;
    color: var(--green);
    text-align: center;
    margin-bottom: 16px;
  }
  .concepts-row {
    display: flex;
    gap: 14px;
    flex-wrap: wrap;
  }
  .concept-box {
    flex: 1 1 180px;
    background: var(--white);
    border: 2px solid var(--border);
    border-top: 4px solid var(--gold);
    border-radius: 6px;
    padding: 16px;
    text-align: center;
  }
  .concept-box .icon { font-size: 1.4rem; display: block; margin-bottom: 6px; }
  .concept-box h3 {
    font-family: Georgia, serif;
    font-size: 0.9rem;
    color: var(--green);
    margin-bottom: 4px;
  }
  .concept-box p {
    font-family: Arial, sans-serif;
    font-size: 0.78rem;
    color: var(--mid);
    line-height: 1.5;
  }

  .callout {
    margin-top: 24px;
    background: var(--green);
    color: var(--white);
    border-radius: 6px;
    padding: 22px 26px;
    display: flex;
    gap: 16px;
    align-items: flex-start;
  }
  .callout-icon { font-size: 1.6rem; flex-shrink: 0; }
  .callout h3 {
    font-family: Georgia, serif;
    font-size: 1rem;
    color: var(--gold);
    margin-bottom: 6px;
  }
  .callout p {
    font-family: Arial, sans-serif;
    font-size: 0.87rem;
    color: #e0e8e4;
    line-height: 1.6;
  }

  .footer {
    text-align: center;
    margin-top: 28px;
    font-family: Arial, sans-serif;
    font-size: 0.73rem;
    color: var(--mid);
  }

  @media (max-width: 580px) {
    .flow-columns { flex-direction: column; min-width: unset; }
    .flow-col:not(:last-child)::after { content: '‚Üì'; right: auto; top: auto; bottom: -18px; left: 50%; transform: translateX(-50%); }
    .phase-strip { flex-direction: column; }
    .concepts-row { flex-direction: column; }
    .callout { flex-direction: column; gap: 10px; }
  }
</style>
</head>
<body>

<a href="#main-content" class="skip-link">Skip to main content</a>

<div class="page" id="main-content">

  <header class="header">
    <h1>How GenAI Works: Putting it Together</h1>
    <p>From building a model to getting a response ‚Äî where each term lives in the process</p>
  </header>

  <p class="hint">Click any term to see its definition and how it connects to other concepts.</p>

  <!-- PHASE LEGEND -->
  <div class="phase-legend" aria-label="Phase color legend">
    <div class="legend-item">
      <div class="legend-dot" style="background:var(--phase-build);"></div>
      <span>Phase 1: Build the Model</span>
    </div>
    <div class="legend-item">
      <div class="legend-dot" style="background:var(--phase-run);"></div>
      <span>Phase 2: Run the Model</span>
    </div>
    <div class="legend-item">
      <div class="legend-dot" style="background:var(--phase-output);"></div>
      <span>Phase 3: Generate Output</span>
    </div>
    <div class="legend-item">
      <div class="legend-dot" style="background:var(--phase-oversight);"></div>
      <span>Throughout: Human Oversight</span>
    </div>
  </div>

  <!-- PHASE STRIP -->
  <div class="phase-strip" aria-hidden="true">
    <div class="phase-label build">Phase 1: Build the Model</div>
    <div class="phase-label run">Phase 2: Run the Model</div>
    <div class="phase-label output">Phase 3: Generate Output</div>
    <div class="phase-label oversight">Throughout</div>
  </div>

  <!-- FLOWCHART -->
  <div class="flowchart-area" role="region" aria-label="AI terms flowchart">
    <div class="flow-columns">

      <!-- PHASE 1: BUILD -->
      <div class="flow-col" aria-label="Phase 1: Build the Model">
        <button class="term-pill build" id="pill-llm" onclick="selectTerm('llm')" aria-pressed="false">
          <span class="pill-phase">Foundation</span>
          Large Language Model (LLM)
        </button>
        <button class="term-pill build" id="pill-training-data" onclick="selectTerm('training-data')" aria-pressed="false">
          <span class="pill-phase">Input</span>
          Training Data
        </button>
        <button class="term-pill build" id="pill-neural-network" onclick="selectTerm('neural-network')" aria-pressed="false">
          <span class="pill-phase">Architecture</span>
          Neural Network
        </button>
        <button class="term-pill build" id="pill-transformer" onclick="selectTerm('transformer')" aria-pressed="false">
          <span class="pill-phase">Architecture</span>
          Transformer
        </button>
        <button class="term-pill build" id="pill-parameters" onclick="selectTerm('parameters')" aria-pressed="false">
          <span class="pill-phase">Learned from data</span>
          Parameters
        </button>
        <button class="term-pill build" id="pill-fine-tuning" onclick="selectTerm('fine-tuning')" aria-pressed="false">
          <span class="pill-phase">Specialization</span>
          Fine-Tuning
        </button>
      </div>

      <!-- PHASE 2: RUN -->
      <div class="flow-col" aria-label="Phase 2: Run the Model">
        <button class="term-pill run" id="pill-prompt" onclick="selectTerm('prompt')" aria-pressed="false">
          <span class="pill-phase">User input</span>
          Prompt
        </button>
        <button class="term-pill run" id="pill-prompt-engineering" onclick="selectTerm('prompt-engineering')" aria-pressed="false">
          <span class="pill-phase">User skill</span>
          Prompt Engineering
        </button>
        <button class="term-pill run" id="pill-tokenization" onclick="selectTerm('tokenization')" aria-pressed="false">
          <span class="pill-phase">Processing step</span>
          Tokenization
        </button>
        <button class="term-pill run" id="pill-token" onclick="selectTerm('token')" aria-pressed="false">
          <span class="pill-phase">Unit of text</span>
          Token
        </button>
        <button class="term-pill run" id="pill-context-window" onclick="selectTerm('context-window')" aria-pressed="false">
          <span class="pill-phase">Memory limit</span>
          Context Window
        </button>
        <button class="term-pill run" id="pill-autoregression" onclick="selectTerm('autoregression')" aria-pressed="false">
          <span class="pill-phase">How it generates</span>
          Autoregression
        </button>
      </div>

      <!-- PHASE 3: OUTPUT -->
      <div class="flow-col" aria-label="Phase 3: Generate Output">
        <button class="term-pill output" id="pill-hallucination" onclick="selectTerm('hallucination')" aria-pressed="false">
          <span class="pill-phase">Risk</span>
          Hallucination
        </button>
      </div>

      <!-- THROUGHOUT: OVERSIGHT -->
      <div class="flow-col" aria-label="Throughout: Human Oversight">
        <button class="term-pill oversight" id="pill-hitl" onclick="selectTerm('hitl')" aria-pressed="false">
          <span class="pill-phase">Responsibility</span>
          Human-in-the-Loop
        </button>
      </div>

    </div>
  </div>

  <!-- DETAIL PANEL -->
  <div class="detail-panel" id="detail-panel" role="region" aria-live="polite" aria-label="Term detail">
    <span class="detail-phase" id="detail-phase-tag"></span>
    <h2 id="detail-title"></h2>
    <p class="definition" id="detail-definition"></p>
    <p class="connections-label">Connects to:</p>
    <p class="connections-list" id="detail-connections"></p>
  </div>

  <!-- KEY TAKEAWAYS -->
  <section class="concepts" aria-labelledby="concepts-heading">
    <h2 id="concepts-heading">Key Takeaways</h2>
    <div class="concepts-row">
      <div class="concept-box">
        <span class="icon" aria-hidden="true">üèóÔ∏è</span>
        <h3>Built, not programmed</h3>
        <p>LLMs learn patterns from training data through a neural network. No one writes the rules by hand.</p>
      </div>
      <div class="concept-box">
        <span class="icon" aria-hidden="true">üî§</span>
        <h3>Tokens, not words</h3>
        <p>Everything you type is broken into tokens before the model processes it. The context window limits how many tokens it can hold at once.</p>
      </div>
      <div class="concept-box">
        <span class="icon" aria-hidden="true">‚ö†Ô∏è</span>
        <h3>Fluency is not accuracy</h3>
        <p>Autoregression produces confident-sounding text. Hallucination is what happens when that confidence outpaces accuracy.</p>
      </div>
    </div>
  </section>

  <footer class="footer">
    <p>Developed with Claude &nbsp;¬∑&nbsp; Baylor University Academy for Teaching &amp; Learning &nbsp;¬∑&nbsp; AI Literacy Course</p>
  </footer>

</div>

<script>
  var terms = {
    'llm': {
      label: 'Large Language Model (LLM)',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'A type of AI system trained on massive amounts of text data to understand and generate human language. The "large" refers to the enormous scale of both the training data and the number of parameters the model learns. LLMs are the foundation everything else in this chart is built upon.',
      connections: ['Training Data (what it learned from)', 'Neural Network (its architecture)', 'Transformer (the specific design that makes modern LLMs powerful)', 'Parameters (what it learned)']
    },
    'training-data': {
      label: 'Training Data',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'The vast collection of text ‚Äî books, articles, websites, conversations ‚Äî that an LLM is exposed to during training. The model learns patterns, facts, and language relationships from this data. The quality and diversity of training data directly shapes what the model knows and where its blind spots are.',
      connections: ['Large Language Model (what the data builds)', 'Parameters (what the model adjusts based on data)', 'Hallucination (gaps or errors in training data can become model errors)', 'Human-in-the-Loop (humans curate and quality-check training data)']
    },
    'neural-network': {
      label: 'Neural Network',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'The mathematical architecture that processes information inside an LLM. Modeled loosely after the human brain, it consists of layers of interconnected nodes (neurons). Each connection has a weight that determines how strongly one node influences another. Patterns are detected as information travels from layer to layer.',
      connections: ['Transformer (the specific neural network design used in LLMs)', 'Parameters (the weights in the neural network)', 'Tokenization (tokens are what enter the neural network)', 'Large Language Model (the neural network is what an LLM is built on)']
    },
    'transformer': {
      label: 'Transformer',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'The specific neural network architecture that powers modern LLMs. Introduced in 2017, transformers use a mechanism called "attention" that allows the model to weigh the relevance of every token in the context window against every other token simultaneously. This is what allows LLMs to understand context and long-range relationships in text.',
      connections: ['Neural Network (transformers are a type of neural network)', 'Token (transformers process tokens, not raw text)', 'Context Window (transformers can attend to everything in the context window)', 'Autoregression (transformers generate output one token at a time)']
    },
    'parameters': {
      label: 'Parameters',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'The billions of numerical values inside a neural network that store what the model has learned. Think of them as the model\'s internal settings. They are not programmed by hand ‚Äî they emerge from training, adjusted billions of times as the model processes training data and refines its predictions. The more parameters, the more the model can learn.',
      connections: ['Neural Network (parameters are the weights in the network)', 'Training Data (parameters are shaped by training data)', 'Large Language Model (the "large" in LLM partly refers to the number of parameters)', 'Hallucination (parameters encode patterns, not facts, which is why errors occur)']
    },
    'fine-tuning': {
      label: 'Fine-Tuning',
      phase: 'build',
      phaseLabel: 'Phase 1: Build the Model',
      definition: 'Additional targeted training given to a pre-trained model using a smaller, more specific dataset. Fine-tuning adapts a general-purpose LLM for a particular task, domain, or audience ‚Äî such as medical, legal, or customer service contexts. It is faster and less expensive than training from scratch because the model already has a strong foundation.',
      connections: ['Large Language Model (fine-tuning adapts a pre-trained LLM)', 'Training Data (fine-tuning uses a smaller, targeted version of training data)', 'Parameters (fine-tuning adjusts parameters for a specific purpose)', 'Human-in-the-Loop (humans design the fine-tuning data and process)']
    },
      label: 'Prompt',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The input you provide to an LLM ‚Äî a question, instruction, or piece of text that the model responds to. The prompt is the starting point for everything the model generates. The model has no memory between conversations, so everything the model needs to know must be in the prompt or context window.',
      connections: ['Prompt Engineering (the skill of crafting effective prompts)', 'Tokenization (your prompt is tokenized before the model processes it)', 'Context Window (your prompt occupies space in the context window)', 'Autoregression (the model generates a response to your prompt one token at a time)']
    },
    'prompt-engineering': {
      label: 'Prompt Engineering',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The practice of designing and refining prompts to get better, more accurate, or more useful responses from an LLM. Because users cannot change a model\'s parameters or training data, the prompt is the primary lever available. Effective prompt engineering includes being specific, providing context, giving examples, and specifying the format you want.',
      connections: ['Prompt (prompt engineering is the skill of writing better prompts)', 'Hallucination (good prompts can reduce but not eliminate hallucination)', 'Human-in-the-Loop (prompt engineering is a key end-user responsibility)', 'Context Window (skilled prompting uses the context window efficiently)']
    },
    'tokenization': {
      label: 'Tokenization',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The process of breaking text into tokens before the model can process it. Tokenization happens before the prompt enters the neural network. It is the bridge between human-readable text and the mathematical representation the model works with.',
      connections: ['Token (tokenization produces tokens)', 'Prompt (the prompt is what gets tokenized)', 'Neural Network (tokens are what enter the network)', 'Context Window (the context window is measured in tokens, not words)']
    },
    'token': {
      label: 'Token',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The basic unit of text that an LLM processes. Tokens are not always whole words ‚Äî they can be parts of words, punctuation, or spaces. On average, one token is roughly three to four characters. The word "tokenization" might be split into two or three tokens. Everything the model reads and writes is measured in tokens.',
      connections: ['Tokenization (the process that creates tokens)', 'Context Window (measured in tokens)', 'Autoregression (the model generates one token at a time)', 'Transformer (transformers process and generate tokens)']
    },
    'context-window': {
      label: 'Context Window',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The maximum amount of text (measured in tokens) that an LLM can process and refer to at one time. This includes your prompt, any documents you provide, and the conversation history. Information outside the context window is invisible to the model. Larger context windows allow the model to work with more information at once.',
      connections: ['Token (the context window is measured in tokens)', 'Prompt (your prompt is part of the context window)', 'Transformer (transformers attend to everything in the context window)', 'Hallucination (the model cannot recall information outside the context window)']
    },
    'autoregression': {
      label: 'Autoregression',
      phase: 'run',
      phaseLabel: 'Phase 2: Run the Model',
      definition: 'The mechanism by which LLMs generate responses ‚Äî one token at a time, with each new token predicted based on all previous tokens. The model does not plan the full response in advance. It samples from a probability distribution of likely next tokens, then repeats the process. This is why responses can vary even to the same prompt.',
      connections: ['Token (autoregression generates one token at a time)', 'Context Window (the model uses the full context window to predict each next token)', 'Hallucination (autoregression can produce confident-sounding but incorrect sequences)', 'Transformer (the transformer architecture powers autoregressive generation)']
    },
    'hallucination': {
      label: 'Hallucination',
      phase: 'output',
      phaseLabel: 'Phase 3: Generate Output',
      definition: 'When an LLM generates information that is factually incorrect, fabricated, or misleading ‚Äî but presented in fluent, confident language. Hallucination occurs because the model generates statistically likely text, not verified facts. It is not lying; it simply has no mechanism to distinguish what is true from what sounds plausible.',
      connections: ['Autoregression (the mechanism that produces hallucinations)', 'Training Data (gaps or errors in training data contribute to hallucination)', 'Parameters (parameters encode patterns, not ground truth)', 'Human-in-the-Loop (critical evaluation of outputs is the primary defense against hallucination)', 'Prompt Engineering (better prompts can reduce but not eliminate hallucination)']
    },
    'hitl': {
      label: 'Human-in-the-Loop',
      phase: 'oversight',
      phaseLabel: 'Throughout: Human Oversight',
      definition: 'The principle that humans must be actively involved at every stage of AI development and use ‚Äî not just as passive recipients of AI output. This includes researchers who curate training data, engineers who set parameters, evaluators who rate model outputs during training, and end users who critically evaluate what the model produces. No stage of the AI pipeline is free from the need for human judgment.',
      connections: ['Training Data (humans curate and quality-check what the model learns from)', 'Parameters (humans design the training process that shapes parameters)', 'Hallucination (humans are the primary defense against hallucination)', 'Prompt Engineering (end users exercise HITL through thoughtful prompting)', 'Large Language Model (HITL applies at every stage of the LLM lifecycle)']
    }
  };

  var currentTerm = null;
  var allIds = Object.keys(terms);

  function selectTerm(id) {
    if (currentTerm === id) {
      clearSelection();
      return;
    }
    currentTerm = id;
    var term = terms[id];

    // Update all pills
    allIds.forEach(function(tid) {
      var pill = document.getElementById('pill-' + tid);
      if (!pill) return;
      pill.classList.remove('active', 'dimmed', 'connected');
      pill.setAttribute('aria-pressed', 'false');

      if (tid === id) {
        pill.classList.add('active');
        pill.setAttribute('aria-pressed', 'true');
      } else {
        // Check if this term is mentioned in connections
        var connText = term.connections.join(' ');
        var termLabel = terms[tid].label.toLowerCase();
        var shortId = tid.replace(/-/g, ' ');
        if (connText.toLowerCase().indexOf(termLabel.split('(')[0].trim().toLowerCase()) !== -1 ||
            connText.toLowerCase().indexOf(shortId) !== -1) {
          pill.classList.add('connected');
        } else {
          pill.classList.add('dimmed');
        }
      }
    });

    // Update detail panel
    var panel = document.getElementById('detail-panel');
    var phaseTag = document.getElementById('detail-phase-tag');
    var titleEl = document.getElementById('detail-title');
    var defEl = document.getElementById('detail-definition');
    var connEl = document.getElementById('detail-connections');

    phaseTag.textContent = term.phaseLabel;
    phaseTag.className = 'detail-phase ' + term.phase;
    titleEl.textContent = term.label;
    defEl.textContent = term.definition;
    connEl.textContent = term.connections.join(' ¬∑ ');
    panel.classList.add('visible');
    panel.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
  }

  function clearSelection() {
    currentTerm = null;
    allIds.forEach(function(tid) {
      var pill = document.getElementById('pill-' + tid);
      if (!pill) return;
      pill.classList.remove('active', 'dimmed', 'connected');
      pill.setAttribute('aria-pressed', 'false');
    });
    document.getElementById('detail-panel').classList.remove('visible');
  }
</script>

</body>
</html>
